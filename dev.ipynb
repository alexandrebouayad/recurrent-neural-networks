{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KAGGLE_USERNAME=\n",
    "%env KAGGLE_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d danielwillgeorge/glove6b100dtxt -p data/ --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 50\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 2048\n",
    "TRAINING_LENGTH = 50\n",
    "TRAIN_FRACTION = 0.7\n",
    "LSTM_CELLS = 64\n",
    "VERBOSE = 1\n",
    "SAVE_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/machine-learning.csv\", parse_dates=['patent_date'])\n",
    "data = data.dropna()\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data[\"year-month\"] = [\n",
    "    datetime(year, month, 1)\n",
    "    for year, month in zip(\n",
    "        data[\"patent_date\"].dt.year,\n",
    "        data[\"patent_date\"].dt.month,\n",
    "    )\n",
    "]\n",
    "monthly = data.groupby(\"year-month\")[\"patent_number\"].count()\n",
    "monthly.plot(figsize=(16, 8))\n",
    "plt.ylabel(\"Number of patents\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.title(\"Machine learning patents over time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"year\"] = [year for year in data[\"patent_date\"].dt.year]\n",
    "yearly = data.groupby(\"year\")[\"patent_number\"].count()\n",
    "yearly.plot.bar(\n",
    "    color=\"red\",\n",
    "    edgecolor=\"k\",\n",
    "    figsize=(16, 8),\n",
    ")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Patents\")\n",
    "plt.title(\"Neural Network patents by year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_patent(patent):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    patent = re.sub(r\"(?<=[^\\s0-9])(?=[.,;?])\", r\" \", patent)\n",
    "\n",
    "    # Remove references to figures\n",
    "    patent = re.sub(r\"\\((\\d+)\\)\", r\"\", patent)\n",
    "\n",
    "    # Remove double spaces\n",
    "    patent = re.sub(r\"\\s\\s\", \" \", patent)\n",
    "    return patent\n",
    "\n",
    "\n",
    "def remove_spaces(patent):\n",
    "    \"\"\"Remove spaces around punctuation\"\"\"\n",
    "    patent = re.sub(r\"\\s+([.,;?])\", r\"\\1\", patent)\n",
    "\n",
    "    return patent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_abstracts = data[\"patent_abstract\"].to_list()\n",
    "formatted_abstracts = []\n",
    "\n",
    "for abstract in original_abstracts:\n",
    "    abstract = format_patent(abstract)\n",
    "    formatted_abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def make_sequences(\n",
    "    texts,\n",
    "    training_length=50,\n",
    "    lower=True,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f\"There are {num_words} unique words.\")\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length : i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f\"There are {len(training_seq)} training sequences.\")\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return (\n",
    "        word_idx,\n",
    "        idx_word,\n",
    "        num_words,\n",
    "        word_counts,\n",
    "        new_texts,\n",
    "        new_sequences,\n",
    "        training_seq,\n",
    "        labels,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "(\n",
    "    word_idx,\n",
    "    idx_word,\n",
    "    num_words,\n",
    "    word_counts,\n",
    "    abstracts,\n",
    "    sequences,\n",
    "    features,\n",
    "    labels,\n",
    ") = make_sequences(\n",
    "    formatted_abstracts,\n",
    "    TRAINING_LENGTH,\n",
    "    lower=True,\n",
    "    filters=filters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def create_train_valid(\n",
    "    features, labels, num_words, train_fraction=TRAIN_FRACTION\n",
    "):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "\n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "\n",
    "    # Memory management\n",
    "    import gc\n",
    "\n",
    "    gc.enable()\n",
    "    del (\n",
    "        features,\n",
    "        labels,\n",
    "        train_features,\n",
    "        valid_features,\n",
    "        train_labels,\n",
    "        valid_labels,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = create_train_valid(\n",
    "    features,\n",
    "    labels,\n",
    "    num_words,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = np.loadtxt(\"data/glove.6B.100d.txt\", dtype='str', comments=None)\n",
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def check_sizes(gb_min=1):\n",
    "    for x in globals():\n",
    "        size = sys.getsizeof(eval(x)) / 1e9\n",
    "        if size > gb_min:\n",
    "            print(f'Object: {x:10}\\tSize: {size} GB.')\n",
    "\n",
    "\n",
    "check_sizes(gb_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for idx, word in idx_word.items():\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[idx, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(query, embedding_matrix, word_idx, idx_word, n=10):\n",
    "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
    "\n",
    "    idx = word_idx.get(query, None)\n",
    "    # Handle case where query is not in vocab\n",
    "    if idx is None:\n",
    "        print(f\"{query} not found in vocab.\")\n",
    "        return\n",
    "    else:\n",
    "        vec = embedding_matrix[idx]\n",
    "        # Handle case where word doesn't have an embedding\n",
    "        if np.all(vec == 0):\n",
    "            print(f\"{query} has no pre-trained embedding.\")\n",
    "            return\n",
    "        else:\n",
    "            # Calculate distance between vector and all others\n",
    "            dists = np.dot(embedding_matrix, vec)\n",
    "\n",
    "            # Sort indexes in reverse order\n",
    "            idxs = np.argsort(dists)[::-1][:n]\n",
    "            sorted_dists = dists[idxs]\n",
    "            closest = [idx_word[i] for i in idxs]\n",
    "\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    # Print out the word and cosine distances\n",
    "    for word, dist in zip(closest, sorted_dists):\n",
    "        print(f\"Word: {word:15} Cosine Similarity: {round(dist, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('the', embedding_matrix, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Masking,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_level_model(\n",
    "    num_words,\n",
    "    embedding_matrix,\n",
    "    lstm_cells=64,\n",
    "    trainable=False,\n",
    "    lstm_layers=1,\n",
    "    bi_direc=False,\n",
    "):\n",
    "    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n",
    "    and varying numbers of LSTM cell layers.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    if not trainable:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                embeddings_initializer=Constant(embedding_matrix),\n",
    "                trainable=False,\n",
    "                mask_zero=True,\n",
    "            )\n",
    "        )\n",
    "        model.add(Masking())\n",
    "    else:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                embeddings_initializer=Constant(embedding_matrix),\n",
    "                trainable=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # If want to add multiple LSTM layers\n",
    "    if lstm_layers > 1:\n",
    "        for _ in range(lstm_layers - 1):\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add final LSTM cell layer\n",
    "    if bi_direc:\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                lstm_cells,\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.1,\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_words, activation=\"softmax\"))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_word_level_model(\n",
    "    num_words,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    lstm_cells=LSTM_CELLS,\n",
    "    trainable=False,\n",
    "    lstm_layers=1,\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "model_name = 'pre-trained-rnn'\n",
    "model_dir = 'models'\n",
    "\n",
    "\n",
    "def make_callbacks(model_name, save=SAVE_MODEL):\n",
    "    \"\"\"Make list of callbacks for training\"\"\"\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_dir}/{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=VERBOSE,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70b6b26dc3583d0ba3de2c2443afa01adb4a07b1a2cce937a9838656e7a215ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
